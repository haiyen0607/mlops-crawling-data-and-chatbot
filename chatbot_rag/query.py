# # import os
# # from dotenv import load_dotenv
# # from datetime import datetime
# # import pandas as pd
# # import google.generativeai as genai

# # from llama_index.core import StorageContext, load_index_from_storage
# # from llama_index.core.settings import Settings
# # from llama_index.embeddings.huggingface import HuggingFaceEmbedding
# # from llama_index.llms.gemini import Gemini

# # # ==== Tá»ª KHÃ“A Äá»‚ PHÃ‚N BIá»†T CÃ‚U Há»ŽI HOT TREND ====
# # TREND_KEYWORDS = [
# #     "chá»§ Ä‘á» hot", "chá»§ Ä‘á» nÃ³ng", "váº¥n Ä‘á» ná»•i cá»™m",
# #     "xu hÆ°á»›ng", "trending", "cá»¥m tá»« hot", "tin nÃ³ng", "váº¥n Ä‘á» Ä‘Ã¡ng chÃº Ã½"
# # ]

# # def is_trend_question(question: str) -> bool:
# #     q = question.lower()
# #     return any(keyword in q for keyword in TREND_KEYWORDS)


# # # ==== Láº¤Y FILE HOT TOPICS Má»šI NHáº¤T ====
# # def get_latest_hot_topics_file(data_dir="data"):
# #     prefix = "hot_topics_"
# #     suffix = ".csv"

# #     files = [
# #         os.path.join(data_dir, f)
# #         for f in os.listdir(data_dir)
# #         if f.startswith(prefix) and f.endswith(suffix)
# #     ]
# #     if not files:
# #         return None

# #     # Sáº¯p xáº¿p theo thá»i gian cáº­p nháº­t gáº§n nháº¥t
# #     files.sort(key=lambda x: os.path.getmtime(x), reverse=True)
# #     return files[0]
# # # ==== IN RA DANH SÃCH HOT TREND ====
# # def show_trending_articles():
# #     file_path = get_latest_hot_topics_file()
# #     if not file_path:
# #         print("KhÃ´ng tÃ¬m tháº¥y dá»¯ liá»‡u hot topics.")
# #         return

# #     df = pd.read_csv(file_path)

# #     print("\nChá»§ Ä‘á» hot gáº§n Ä‘Ã¢y vÃ  bÃ i viáº¿t liÃªn quan:")
# #     for _, row in df.iterrows():
# #         print(f"- [{row['topic']}] {row['url']} ({row['created_at']})")


# # # Load API Key
# # os.environ["GOOGLE_API_KEY"] = "AIzaSyD10Ny4Gd9nKBY-irrQ70RNQeiHVuBCkGM"

# # # Load embedding
# # embed_model = HuggingFaceEmbedding(model_name="bkai-foundation-models/vietnamese-bi-encoder")
# # Settings.embed_model = embed_model

# # # Load LLM
# # llm = Gemini(
# #     model_name="models/gemini-1.5-flash",
# #     api_key=os.environ["GOOGLE_API_KEY"]
# # )
# # Settings.llm = llm

# # # Load index tá»« thÆ° má»¥c Ä‘Ã£ lÆ°u
# # storage_context = StorageContext.from_defaults(persist_dir="index/storage")
# # index = load_index_from_storage(storage_context)

# # # Táº¡o Query Engine
# # query_engine = index.as_query_engine(llm=llm, similarity_top_k=10)

# # # ==== VÃ’NG Láº¶P CHAT ====
# # while True:
# #     question = input("Nháº­p cÃ¢u há»i (gÃµ 'exit' Ä‘á»ƒ thoÃ¡t):\n>> ")
# #     if question.lower() in ["exit", "quit"]:
# #         break

# #     if is_trend_question(question):
# #         show_trending_articles()
# #         continue

# #     response = query_engine.query(question)

# #     # Sáº¯p xáº¿p káº¿t quáº£ theo ngÃ y má»›i nháº¥t
# #     nodes = response.source_nodes
# #     try:
# #         for node in nodes:
# #             if isinstance(node.metadata.get("created_at"), str):
# #                 node.metadata["created_at"] = datetime.strptime(node.metadata["created_at"], "%Y-%m-%d")
# #         sorted_nodes = sorted(nodes, key=lambda x: x.metadata["created_at"], reverse=True)

# #         print("\nCÃ¢u tráº£ lá»i:")
# #         for node in sorted_nodes:
# #             date = node.metadata["created_at"].strftime("%Y-%m-%d")
# #             title = node.metadata.get("title", "KhÃ´ng cÃ³ tiÃªu Ä‘á»")
# #             print(f"{date} - {title}")
# #     except Exception as e:
# #         print("ÄÃ£ xáº£y ra lá»—i khi sáº¯p xáº¿p theo ngÃ y:", e)
# #         print("\nCÃ¢u tráº£ lá»i thÃ´:\n", response)

# import os
# from datetime import datetime
# from llama_index.core import StorageContext, load_index_from_storage
# from llama_index.core.settings import Settings
# from llama_index.embeddings.huggingface import HuggingFaceEmbedding
# from llama_index.llms.gemini import Gemini

# # Thiáº¿t láº­p API key (hoáº·c load tá»« .env náº¿u cáº§n)
# os.environ["GOOGLE_API_KEY"] = "AIzaSyAm9RzA3ckL-pHO25PgeWihdRvVkPsRaIY"

# # ÄÆ°á»ng dáº«n tuyá»‡t Ä‘á»‘i Ä‘áº¿n index
# index_path = os.path.join(os.path.dirname(__file__), "index", "storage")
# storage_context = StorageContext.from_defaults(persist_dir=index_path)

# # Thiáº¿t láº­p mÃ´ hÃ¬nh
# embed_model = HuggingFaceEmbedding(model_name="bkai-foundation-models/vietnamese-bi-encoder")
# Settings.embed_model = embed_model

# llm = Gemini(model_name="models/gemini-1.5-flash", api_key=os.environ["GOOGLE_API_KEY"])
# Settings.llm = llm

# # Load index
# storage_context = StorageContext.from_defaults(persist_dir=index_path)
# index = load_index_from_storage(storage_context)
# query_engine = index.as_query_engine(llm=llm, similarity_top_k=30)


# # HÃ€M CHO STREAMLIT Gá»ŒI
# def run_query_external(question: str, top_k: int = 4):
#     try:
#         response = query_engine.query(question)
#         nodes = response.source_nodes

#         for node in nodes:
#             if isinstance(node.metadata.get("created_at"), str):
#                 node.metadata["created_at"] = datetime.strptime(node.metadata["created_at"], "%Y-%m-%d")

#         sorted_nodes = sorted(nodes, key=lambda x: x.metadata["created_at"], reverse=True)
#         seen = set()
#         result = []

#         for node in sorted_nodes:
#             title = node.metadata.get("title", "KhÃ´ng cÃ³ tiÃªu Ä‘á»")
#             url = node.metadata.get("url", "")
#             date = node.metadata["created_at"].strftime("%Y-%m-%d")
#             if title not in seen:
#                 seen.add(title)
#                 result.append(f"{date} - [{title}]({url})")
#             if len(result) >= top_k:
#                 break

#         return result
#     except Exception as e:
#         return [f"Lá»—i: {str(e)}"]


# # PHáº¦N CLI CHá»ˆ CHáº Y KHI RUN TRá»°C TIáº¾P
# if __name__ == "__main__":
#     while True:
#         question = input("Nháº­p cÃ¢u há»i (gÃµ 'exit' Ä‘á»ƒ thoÃ¡t):\n>> ")
#         if question.lower() in ["exit", "quit"]:
#             break

#         results = run_query_external(question)
#         print("\nCÃ¡c bÃ i viáº¿t liÃªn quan:")
#         for line in results:
#             print(f"- {line}")


import os
from datetime import datetime
from llama_index.core import StorageContext, load_index_from_storage
from llama_index.core.settings import Settings
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.llms.gemini import Gemini

# Thiáº¿t láº­p API key
#AIzaSyBpnsFVe2Fn_YiVY67X4dCUIkxj5FuZhjA
os.environ["GOOGLE_API_KEY"] = "AIzaSyDstywi_yI7m8QYHvqLF7SOEVxx_4FmShc"

# ÄÆ°á»ng dáº«n tuyá»‡t Ä‘á»‘i Ä‘áº¿n index
index_path = os.path.join(os.path.dirname(__file__), "index", "storage")

# Thiáº¿t láº­p mÃ´ hÃ¬nh embedding vÃ  LLM
embed_model = HuggingFaceEmbedding(model_name="bkai-foundation-models/vietnamese-bi-encoder")
Settings.embed_model = embed_model

llm = Gemini(model_name="models/gemini-1.5-flash", api_key=os.environ["GOOGLE_API_KEY"])
Settings.llm = llm

# HÃ€M CHO STREAMLIT Gá»ŒI
def run_query_external(question: str, top_k: int = 4):
    try:
        # ðŸ” LuÃ´n load láº¡i index má»—i láº§n gá»i
        storage_context = StorageContext.from_defaults(persist_dir=index_path)
        index = load_index_from_storage(storage_context)
        query_engine = index.as_query_engine(llm=llm, similarity_top_k=30)

        response = query_engine.query(question)
        nodes = response.source_nodes

        for node in nodes:
            if isinstance(node.metadata.get("created_at"), str):
                node.metadata["created_at"] = datetime.strptime(node.metadata["created_at"], "%Y-%m-%d")

        sorted_nodes = sorted(nodes, key=lambda x: x.metadata["created_at"], reverse=True)
        seen = set()
        result = []

        for node in sorted_nodes:
            title = node.metadata.get("title", "KhÃ´ng cÃ³ tiÃªu Ä‘á»")
            url = node.metadata.get("url", "")
            date = node.metadata["created_at"].strftime("%Y-%m-%d")
            if title not in seen:
                seen.add(title)
                result.append(f"{date} - [{title}]({url})")
            if len(result) >= top_k:
                break

        return result
    except Exception as e:
        return [f"Lá»—i: {str(e)}"]


# PHáº¦N CLI CHá»ˆ CHáº Y KHI RUN TRá»°C TIáº¾P
if __name__ == "__main__":
    while True:
        question = input("Nháº­p cÃ¢u há»i (gÃµ 'exit' Ä‘á»ƒ thoÃ¡t):\n>> ")
        if question.lower() in ["exit", "quit"]:
            break

        results = run_query_external(question)
        print("\nCÃ¡c bÃ i viáº¿t liÃªn quan:")
        for line in results:
            print(f"- {line}")
