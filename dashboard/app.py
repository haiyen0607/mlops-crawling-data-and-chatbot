import streamlit as st
import pandas as pd
import altair as alt
from streamlit.components.v1 import html
import os
from datetime import datetime
import glob
from PIL import Image
import json
import sys

st.set_page_config(layout="wide")

# ---------- WebSocket Client ----------
html("""
<script>
(function() {
  function connectWS() {
    const socket = new WebSocket("ws://127.0.0.1:8000/ws");
    socket.onmessage = function(event) {
      if (event.data.trim() === "data_updated") {
        parent.window.location.reload();
      }
    };
    socket.onopen = function() {
      console.log("‚úÖ WebSocket connected");
    };
    socket.onerror = function(err) {
      console.error("WebSocket error:", err);
    };
    socket.onclose = function() {
      console.warn("WebSocket closed, retrying in 3s...");
      setTimeout(connectWS, 3000);
    };
  }
  window.addEventListener("load", connectWS);
})();
</script>
""", height=0, scrolling=False)

# ---------- D√¢n Tr√≠ Section ----------
st.header("üìä Dashboard b√†i vi·∫øt D√¢n Tr√≠ - T·ª± ƒë·ªông c·∫≠p nh·∫≠t")
try:
    df_dantri_raw = pd.read_csv("../crawl_dantri/data/data.csv")
except Exception as e:
    st.error(f"Kh√¥ng th·ªÉ ƒë·ªçc d·ªØ li·ªáu D√¢n Tr√≠: {e}")
    df_dantri_raw = pd.DataFrame()

if not df_dantri_raw.empty:
    df_dantri = df_dantri_raw.dropna(subset=["title", "url", "content"]).copy()
    df_dantri["topic"] = df_dantri["url"].str.extract(r"dantri.com.vn/([^/]+)/")

    st.metric("S·ªë d√≤ng th·ª±c t·∫ø ƒë·ªçc ƒë∆∞·ª£c (D√¢n Tr√≠)", df_dantri_raw.shape[0])
    st.metric("T·ªïng s·ªë b√†i vi·∫øt (D√¢n Tr√≠)", len(df_dantri))

    st.subheader("M·ªôt s·ªë b√†i vi·∫øt ƒë·∫ßu ti√™n")
    st.dataframe(df_dantri[["title", "url"]].head(), use_container_width=True)

    summary_dt = df_dantri['topic'].value_counts().reset_index()
    summary_dt.columns = ['Chuy√™n m·ª•c', 'S·ªë b√†i']
    summary_dt['T·ªâ l·ªá (%)'] = (summary_dt['S·ªë b√†i'] / len(df_dantri) * 100).round(2)

    st.subheader("T·ª∑ l·ªá chuy√™n m·ª•c (D√¢n Tr√≠)")
    st.dataframe(summary_dt, use_container_width=True)

    st.altair_chart(alt.Chart(summary_dt).mark_bar().encode(
        x='S·ªë b√†i:Q', y=alt.Y('Chuy√™n m·ª•c:N', sort='-x'),
        color='Chuy√™n m·ª•c:N', tooltip=['Chuy√™n m·ª•c', 'S·ªë b√†i', 'T·ªâ l·ªá (%)']
    ).properties(width=700, height=400), use_container_width=True)

    st.altair_chart(alt.Chart(summary_dt.sort_values("S·ªë b√†i", ascending=False)).mark_line(point=True).encode(
        x=alt.X('Chuy√™n m·ª•c:N', sort=summary_dt.sort_values("S·ªë b√†i", ascending=False)['Chuy√™n m·ª•c'].tolist(), title='Chuy√™n m·ª•c'),
        y=alt.Y('T·ªâ l·ªá (%):Q', title='T·ªâ l·ªá (%)'),
        tooltip=['Chuy√™n m·ª•c', 'T·ªâ l·ªá (%)']
    ).properties(width=900, height=400), use_container_width=True)

    # Hi·ªÉn th·ªã l·ªãch s·ª≠ c·∫≠p nh·∫≠t cho D√¢n Tr√≠
    st.subheader("L·ªãch s·ª≠ c·∫≠p nh·∫≠t (D√¢n Tr√≠)")
    log_file_dt = "../crawl_dantri/data/update_log.txt"
    topic_history_file_dt = "../crawl_dantri/data/topic_history.csv"
    now_dt = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    total_articles_dt = len(df_dantri)

    log_line_dt = f"{now_dt} | T·ªïng b√†i vi·∫øt: {total_articles_dt}"
    os.makedirs(os.path.dirname(log_file_dt), exist_ok=True)
    with open(log_file_dt, mode="a", encoding="utf-8") as f:
        f.write(log_line_dt + "\n")

    summary_dt['timestamp'] = now_dt
    write_header_dt = not os.path.exists(topic_history_file_dt)
    summary_dt[["timestamp", "Chuy√™n m·ª•c", "T·ªâ l·ªá (%)"]].rename(columns={
        "Chuy√™n m·ª•c": "topic",
        "T·ªâ l·ªá (%)": "ratio"
    }).to_csv(topic_history_file_dt, mode='a', index=False, header=write_header_dt, encoding='utf-8')

    try:
        with open(log_file_dt, mode="r", encoding="utf-8") as f:
            logs_dt = [line.strip().split(" | ") for line in f.readlines()][-10:]
            df_logs_dt = pd.DataFrame(logs_dt, columns=["Th·ªùi ƒëi·ªÉm c·∫≠p nh·∫≠t", "T·ªïng b√†i vi·∫øt"])
            st.table(df_logs_dt)
    except Exception as e:
        st.warning("Kh√¥ng th·ªÉ ƒë·ªçc l·ªãch s·ª≠ c·∫≠p nh·∫≠t (D√¢n Tr√≠).")

# ---------- VnExpress Section ----------
st.header("üìä Dashboard b√†i vi·∫øt VnExpress - T·ª± ƒë·ªông c·∫≠p nh·∫≠t")
try:
    df_vne_raw = pd.read_csv("../crawl_vnexpress/data_vnexpress/data.csv")
except Exception as e:
    st.error(f"Kh√¥ng th·ªÉ ƒë·ªçc d·ªØ li·ªáu VnExpress: {e}")
    df_vne_raw = pd.DataFrame()

if not df_vne_raw.empty:
    df_vne = df_vne_raw.dropna(subset=["title", "url", "content", "topic"]).copy()

    st.metric("S·ªë d√≤ng th·ª±c t·∫ø ƒë·ªçc ƒë∆∞·ª£c (VnExpress)", df_vne_raw.shape[0])
    st.metric("T·ªïng s·ªë b√†i vi·∫øt (VnExpress)", len(df_vne))

    st.subheader("M·ªôt s·ªë b√†i vi·∫øt ƒë·∫ßu ti√™n (VnExpress)")
    st.dataframe(df_vne[["title", "url"]].head(), use_container_width=True)

    summary_vne = df_vne['topic'].value_counts().reset_index()
    summary_vne.columns = ['Chuy√™n m·ª•c', 'S·ªë b√†i']
    summary_vne['T·ªâ l·ªá (%)'] = (summary_vne['S·ªë b√†i'] / len(df_vne) * 100).round(2)

    st.subheader("T·ª∑ l·ªá chuy√™n m·ª•c (VnExpress)")
    st.dataframe(summary_vne, use_container_width=True)

    st.altair_chart(alt.Chart(summary_vne).mark_bar().encode(
        x='S·ªë b√†i:Q', y=alt.Y('Chuy√™n m·ª•c:N', sort='-x'),
        color='Chuy√™n m·ª•c:N', tooltip=['Chuy√™n m·ª•c', 'S·ªë b√†i', 'T·ªâ l·ªá (%)']
    ).properties(width=700, height=400), use_container_width=True)

    st.altair_chart(alt.Chart(summary_vne.sort_values("T·ªâ l·ªá (%)", ascending=False)).mark_line(point=True).encode(
        x=alt.X('Chuy√™n m·ª•c:N', sort=summary_vne.sort_values("T·ªâ l·ªá (%)", ascending=False)['Chuy√™n m·ª•c'].tolist(), title='Chuy√™n m·ª•c'),
        y=alt.Y('T·ªâ l·ªá (%):Q', title='T·ªâ l·ªá (%)'),
        tooltip=['Chuy√™n m·ª•c', 'T·ªâ l·ªá (%)']
    ).properties(width=900, height=400), use_container_width=True)

    # Hi·ªÉn th·ªã l·ªãch s·ª≠ c·∫≠p nh·∫≠t cho VnExpress
    st.subheader("L·ªãch s·ª≠ c·∫≠p nh·∫≠t (VnExpress)")
    log_file_vne = "../crawl_vnexpress/data_vnexpress/update_log.txt"
    topic_history_file_vne = "../crawl_vnexpress/data_vnexpress/topic_history.csv"
    now_vne = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    total_articles_vne = len(df_vne)

    log_line_vne = f"{now_vne} | T·ªïng b√†i vi·∫øt: {total_articles_vne}"
    os.makedirs(os.path.dirname(log_file_vne), exist_ok=True)
    with open(log_file_vne, mode="a", encoding="utf-8") as f:
        f.write(log_line_vne + "\n")

    summary_vne['timestamp'] = now_vne
    write_header_vne = not os.path.exists(topic_history_file_vne)
    summary_vne[["timestamp", "Chuy√™n m·ª•c", "T·ªâ l·ªá (%)"]].rename(columns={
        "Chuy√™n m·ª•c": "topic",
        "T·ªâ l·ªá (%)": "ratio"
    }).to_csv(topic_history_file_vne, mode='a', index=False, header=write_header_vne, encoding='utf-8')

    try:
        with open(log_file_vne, mode="r", encoding="utf-8") as f:
            logs_vne = [line.strip().split(" | ") for line in f.readlines()][-10:]
            df_logs_vne = pd.DataFrame(logs_vne, columns=["Th·ªùi ƒëi·ªÉm c·∫≠p nh·∫≠t", "T·ªïng b√†i vi·∫øt"])
            st.table(df_logs_vne)
    except Exception as e:
        st.warning("Kh√¥ng th·ªÉ ƒë·ªçc l·ªãch s·ª≠ c·∫≠p nh·∫≠t (VnExpress).")

# ---------- Ph√¢n ph·ªëi c·ª•m t·ª´ hot (D√¢n Tr√≠) ----------
st.header("üî• Bi·ªÉu ƒë·ªì c·ª•m t·ª´ hot trong 3 ng√†y g·∫ßn nh·∫•t")

try:
    # JSON ph√¢n ph·ªëi c·ª•m t·ª´
    with open("../analyze_dantri_trends/data/top_phrases.json", "r", encoding="utf-8") as f:
        top_phrases = json.load(f)
    df_phrases = pd.DataFrame(top_phrases, columns=["C·ª•m t·ª´", "S·ªë l·∫ßn xu·∫•t hi·ªán"])

    st.dataframe(df_phrases, use_container_width=True)

    st.altair_chart(alt.Chart(df_phrases).mark_bar().encode(
        x=alt.X("S·ªë l·∫ßn xu·∫•t hi·ªán:Q", title="S·ªë l·∫ßn xu·∫•t hi·ªán"),
        y=alt.Y("C·ª•m t·ª´:N", sort='-x', title="C·ª•m t·ª´"),
        tooltip=["C·ª•m t·ª´", "S·ªë l·∫ßn xu·∫•t hi·ªán"]
    ).properties(width=750, height=400), use_container_width=True)

    # Hi·ªÉn th·ªã ·∫£nh bi·ªÉu ƒë·ªì m·ªõi nh·∫•t
    image_dir = "../analyze_dantri_trends/data/image/"
    image_files = sorted(glob.glob(os.path.join(image_dir, "top_phrases_*.png")), reverse=True)
    if image_files:
        latest_img = image_files[0]
        st.image(Image.open(latest_img), caption=f"·∫¢nh: {os.path.basename(latest_img)}", use_container_width=True)

except Exception as e:
    st.warning(f"Kh√¥ng th·ªÉ ƒë·ªçc bi·ªÉu ƒë·ªì ph√¢n ph·ªëi c·ª•m t·ª´: {e}")

# ---------- Hi·ªÉn th·ªã b·∫£ng ch·ªß ƒë·ªÅ hot v√† b√†i vi·∫øt li√™n quan ----------
st.subheader("üî• Ch·ªß ƒë·ªÅ hot nh·∫•t g·∫ßn ƒë√¢y v√† b√†i vi·∫øt li√™n quan")

try:
    topic_files = sorted(glob.glob("../analyze_dantri_trends/data/hot_topics_*.csv"), reverse=True)
    if topic_files:
        latest_topic_file = topic_files[0]
        df_topic = pd.read_csv(latest_topic_file)
        st.dataframe(df_topic, use_container_width=True)
    else:
        st.info("Kh√¥ng t√¨m th·∫•y file hot_topics_*.csv")

except Exception as e:
    st.warning(f"L·ªói khi ƒë·ªçc hot_topics.csv: {e}")

# ---------------chatbot----------------
sys.path.append(os.path.abspath("../chatbot_rag"))

from query import run_query_external 

st.header("Chatbot h·ªèi ƒë√°p b√°o ch√≠")

with st.expander("H·ªèi chatbot v·ªÅ d·ªØ li·ªáu b√°o ch√≠"):
    user_question = st.text_input("Nh·∫≠p c√¢u h·ªèi:")

    if user_question:
        with st.spinner("ƒêang x·ª≠ l√Ω..."):
            answers = run_query_external(user_question)
            for line in answers:
                st.markdown(f"- {line}")

